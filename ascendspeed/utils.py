import torch
from megatron import get_args
from megatron.core import mpu


def get_batch_on_this_cp_rank(batch):
    """ Slice batch input along sequence dimension into multiple chunks,
        which are parallelized across GPUs in a context parallel group.
    """

    # With causal masking, each token only attends to its prior tokens. Simply split
    # sequence into CP chunks can result in severe load imbalance. That's to say, chunks
    # at the end of sequence have bigger workload than others. To address this issue,
    # we split sequence into 2*CP ranks. Assuming CP=2, we then get 4 chunks, chunk_0
    # and chunk_3 are assigned to GPU0, chunk_1 and chunk_2 are assigned to GPU1, so
    # that we can get balanced workload among GPUs in a context parallel group.
    args = get_args()
    cp_size = args.context_parallel_size
    if cp_size > 1 and args.context_parallel_algo == 'megatron_cp_algo':
        cp_rank = mpu.get_context_parallel_rank()
        for key, val in batch.items():
            seq_dim = 1 if key != 'attention_mask' else 2
            val = val.view(
                *val.shape[0:seq_dim],
                2 * cp_size,
                val.shape[seq_dim] // (2 * cp_size),
                *val.shape[(seq_dim + 1):],
            )
            index = torch.tensor([cp_rank, (2 * cp_size - cp_rank - 1)], device=val.device)
            val = val.index_select(seq_dim, index)
            val = val.view(*val.shape[0:seq_dim], -1, *val.shape[(seq_dim + 2):])
            batch[key] = val
    elif cp_size > 1 and args.context_parallel_algo == 'ulysses_cp_algo':
        cp_rank = mpu.get_context_parallel_rank()
        cp_size = mpu.get_context_parallel_world_size()
        for key, val in batch.items():
            if key == 'attention_mask':
                continue
            if val is not None:
                seq_dim = 1
                old_seq_len = val.shape[seq_dim]
                new_seq_len = old_seq_len // cp_size
                val = val[:, new_seq_len * cp_rank:new_seq_len * (cp_rank + 1)]
                batch[key] = val

    return batch
